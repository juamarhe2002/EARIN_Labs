# -*- coding: utf-8 -*-
"""EARIN_Lab5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M2RowZUFf8bl3qgp_6b6JJQWhXOFEk9f

# EARIN Lab 5 code implementation

## Importing libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch import nn
from torch.optim import SGD
from torchvision.transforms import transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from PIL import Image
import pandas as pd
import numpy as np
import os

device = 'cuda' if torch.cuda.is_available() else 'cpu'

torch.manual_seed(2)
torch.use_deterministic_algorithms(mode=True)
np.random.seed(2)
# %env CUBLAS_WORKSPACE_CONFIG=:4096:8

"""## Downloading and importing the dataset."""

transform = transforms.Compose([transforms.Resize((28, 28)), transforms.ToTensor(), transforms.ConvertImageDtype(torch.float)])

from torchvision import datasets
train_dataset = datasets.FashionMNIST(root='./train_data', train=True, download=True, transform=transform)
val_dataset = datasets.FashionMNIST(root='./val_data', train=False, download=True, transform=transform)

inverse_encode_labels = [
    'T-shirt/top', 'Trousers', 'Pullover',
    'Dress', 'Coat', 'Sandal', 'Shirt',
    'Sneaker', 'Bag', 'Ankle boot'
    ]

LR_1 = 1e-3
BATCH_1 = 16
EPOCHS = 20

train_loader = DataLoader(train_dataset, batch_size = BATCH_1, shuffle = True)
val_loader = DataLoader(val_dataset, batch_size = BATCH_1, shuffle = True)

"""## Neural Network"""

class NetworkModel_1(nn.Module):
  def __init__(self, ini_hidden = 14, activation_func = nn.ReLU()):
    super().__init__()
    self.ini_hidden = ini_hidden

    self.conv1 = nn.Conv2d(1, self.ini_hidden, kernel_size = 3, padding=1)
    self.conv2 = nn.Conv2d(self.ini_hidden, 2*self.ini_hidden, kernel_size = 3, padding=1)

    self.pooling = nn.MaxPool2d(2, 2)

    self.activation_func = activation_func

    self.flatten = nn.Flatten()
    self.linear = nn.Linear((2*self.ini_hidden*7*7), self.ini_hidden)

    self.output = nn.Linear(self.ini_hidden, len(inverse_encode_labels))

  def forward(self, x):
                        # Outputs
    x = self.conv1(x)   # -> (14, 28, 28)
    x = self.activation_func(x)
    x = self.pooling(x) # -> (14, 14, 14)

    x = self.conv2(x)   # -> (28, 14, 14)
    x = self.activation_func(x)
    x = self.pooling(x) # -> (28, 7, 7)

    x = self.flatten(x) # -> (28 * 7 * 7)
    x = self.linear(x)  # -> 28
    x = self.output(x)  # -> 10
    return x

def train_loop(model, criterion, optimizer, train_loader, val_loader):
  total_loss_train_plot = []
  total_loss_val_plot = []
  total_loss_batch_plot = []
  total_acc_train_plot = []
  total_acc_val_plot = []

  for epoch in range(EPOCHS):
    total_loss_train = 0
    total_loss_val = 0
    total_acc_train = 0
    total_acc_val = 0

    for images, labels in train_loader:
      images, labels = images.to(device), labels.to(device)

      # Forward pass
      predictions = model(images)
      train_loss = criterion(predictions, labels)
      total_loss_train += train_loss.item()
      total_loss_batch_plot.append(train_loss.item())

      train_acc = (torch.argmax(predictions, axis=1) == labels).sum().item()
      total_acc_train += train_acc

      # Backward pass
      optimizer.zero_grad()
      train_loss.backward()
      optimizer.step()

    with torch.no_grad():
      for images, labels in val_loader:
        images, labels = images.to(device), labels.to(device)
        predictions = model(images)
        val_loss = criterion(predictions, labels)
        total_loss_val += val_loss.item()

        val_acc = (torch.argmax(predictions, axis=1) == labels).sum().item()
        total_acc_val += val_acc

    total_loss_train_plot.append(round(total_loss_train / 1000, 4))
    total_acc_train_plot.append(round((total_acc_train / train_dataset.__len__()) * 100, 4))

    total_loss_val_plot.append(round(total_loss_val / 1000, 4))
    total_acc_val_plot.append(round((total_acc_val / val_dataset.__len__()) * 100, 4))

    print(f"""
    Epoch {epoch + 1}/{EPOCHS}
          Train Loss: {round(total_loss_train / 1000, 4)}
          Validation Loss: {round(total_loss_val / 1000, 4)}
          Train Accuracy: {round((total_acc_train / train_dataset.__len__()) * 100, 4)}
          Validation Accuracy: {round((total_acc_val / val_dataset.__len__()) * 100, 4)}
    """)
    print("="*40)

  return total_loss_train_plot, total_acc_train_plot, total_loss_val_plot, total_acc_val_plot, total_loss_batch_plot

def plot_train_val(total_loss_train_plot, total_loss_val_plot, total_acc_train_plot, total_acc_val_plot):
  fig, axis = plt.subplots(nrows = 1, ncols = 2, figsize=(15,5))

  axis[0].plot(total_loss_train_plot, label="Training Loss")
  axis[0].plot(total_loss_val_plot, label="Validation Loss")
  axis[0].set_title("Training and Validation Loss")
  axis[0].set_xlabel("Epochs")
  axis[0].set_ylabel("Loss")
  axis[0].legend()

  axis[1].plot(total_acc_train_plot, label="Training Accuracy")
  axis[1].plot(total_acc_val_plot, label="Validation Accuracy")
  axis[1].set_title("Training and Validation Accuracy")
  axis[1].set_xlabel("Epochs")
  axis[1].set_ylabel("Accuracy")
  axis[1].legend()

  plt.show()

def plot_batch_loss(total_loss_batch_plot):
  plt.figure(figsize=(15, 5))

  plt.plot(total_loss_batch_plot, label='Batch loss')
  plt.title('Batch loss for every learning step')
  plt.xlabel('Steps')
  plt.ylabel('Loss')

  plt.show()

"""## Experiments

### Experiment 1
"""

model_1 = NetworkModel_1().to(device)

from torchsummary import summary
summary(model_1, input_size=(1, 28, 28))

criterion = nn.CrossEntropyLoss()
optimizer = SGD(model_1.parameters(), lr = LR_1)

train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_1, criterion, optimizer, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 2"""

model_2 = NetworkModel_1().to(device)
optimizer_2 = SGD(model_2.parameters(), lr=0.01)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_2, criterion, optimizer_2, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 3"""

model_3 = NetworkModel_1().to(device)
optimizer_3 = SGD(model_3.parameters(), lr=0.0001)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_3, criterion, optimizer_3, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 4"""

BATCH_SIZE = 1
train_loader_1 = DataLoader(train_dataset, batch_size=BATCH_SIZE)
val_loader_1 = DataLoader(val_dataset, batch_size=BATCH_SIZE)
model_4 = NetworkModel_1().to(device)
optimizer_4 = SGD(model_4.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_4, criterion, optimizer_4, train_loader_1, val_loader_1)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 5"""

BATCH_SIZE = 64
train_loader_64 = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader_64 = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)
model_5 = NetworkModel_1().to(device)
optimizer_5 = SGD(model_5.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_5, criterion, optimizer_5, train_loader_64, val_loader_64)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 6"""

model_6 = NetworkModel_1(7).to(device)
optimizer_6 = SGD(model_6.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_6, criterion, optimizer_6, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 7"""

model_7 = NetworkModel_1(28).to(device)
optimizer_7 = SGD(model_7.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_7, criterion, optimizer_7, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 8"""

model_8 = NetworkModel_1(14, nn.Sigmoid()).to(device)
optimizer_8 = SGD(model_8.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_8, criterion, optimizer_8, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 9"""

model_9 = NetworkModel_1(14, nn.GELU()).to(device)
optimizer_9 = SGD(model_9.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_9, criterion, optimizer_9, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 10"""

class NetworkModel_2(nn.Module):
  def __init__(self):
    super().__init__()

    self.conv1 = nn.Conv2d(1, 28, kernel_size = 3, padding=1)

    self.pooling = nn.MaxPool2d(2, 2)

    self.relu = nn.ReLU()

    self.flatten = nn.Flatten()
    self.linear1 = nn.Linear((28*14*14), 28)
    self.output = nn.Linear(28, len(inverse_encode_labels))

  def forward(self, x):
                        # Outputs
    x = self.conv1(x)   # -> (28, 28, 28)
    x = self.relu(x)
    x = self.pooling(x) # -> (28, 14, 14)

    x = self.flatten(x) # -> 28 * 14 * 14

    x = self.linear1(x) # -> 28
    x = self.output(x)  # -> 10
    return x

model_10 =  NetworkModel_2().to(device)
optimizer_10 = SGD(model_10.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_10, criterion, optimizer_10, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)

"""### Experiment 11"""

class NetworkModel_3(nn.Module):
  def __init__(self):
    super().__init__()
    self.flatten = nn.Flatten()
    self.output = nn.Linear((1*28*28), len(inverse_encode_labels))

  def forward(self, x):
    x = self.flatten(x) # -> 1 * 28 * 28
    x = self.output(x)  # -> 10
    return x

model_11 =  NetworkModel_3().to(device)
optimizer_11 = SGD(model_11.parameters(), lr=LR_1)
train_loss, train_acc, val_loss, val_acc, batch_loss = train_loop(model_11, criterion, optimizer_11, train_loader, val_loader)

plot_train_val(train_loss, val_loss, train_acc, val_acc)

plot_batch_loss(batch_loss)